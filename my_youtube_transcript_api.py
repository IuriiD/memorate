# This is not an official approach, captions are fetched without authorization
# Is used for POC only
# Example: https://www.youtube.com/watch?v=L8U-pm-vZ4c
# Autoplay between start and stop time, show captions
# https://www.youtube.com/embed/L8U-pm-vZ4c?start=60&end=70&autoplay=1&hl=en&cc_lang_pref=en

from youtube_transcript_api import YouTubeTranscriptApi
import json

def get_transcript(video_id, lang_code='en'):
  try:
    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
    en_transcript = transcript_list.find_transcript([lang_code])
    if not en_transcript:
      print('[getYoutubeCaptions] No English transcript found, halting')
      return None
    transcript = en_transcript.fetch()
    print(f'''[getYoutubeCaptions] Got Youtube captions for the video {video_id}:
    {transcript}''')
    return transcript
  except Exception as e:
    print(f'[getYoutubeCaptions] Error getting youtube captions: ', e)
    return None

def join_yt_transcripts(transcripts):
    text_only = [piece['text'] for piece in transcripts]
    return ' '.join(text_only)

# t = get_transcript('L8U-pm-vZ4c')
# print(join_yt_transcripts(t))

url = 'https://www.youtube.com/watch?v=L8U-pm-vZ4c'

def get_yuotube_video_id(youtube_url):
  return youtube_url.split('v=')[1]

# print(get_yuotube_video_id(url))

# t = get_transcript(get_yuotube_video_id(url))
# print(t)
# print('\n')
# print(join_yt_transcripts(t))

joined_text = "today we're going to talk about abstractive or generative question and answering and we're going to focus on actually building or implementing something like this using a few different components but in the end what we're going to essentially be able to get is we're going to be able to ask a question in natural language and we're going to be able to return documents or web pages or so on that are related to our particular question and we're also going to be able to use something called a generator model to generate a human natural language answer to our question based on these documents that we've retrieved from an external source so we can think of it as a GPT model that is answering our questions but if gbt was also giving us the sources of the information that it was answering questions based on so let's jump straight into just understanding why exactly that we're going to be building so we're going to start with all of our documents our text or whatever it is we're going to be using in our case we're going to be using text from Wikipedia so we're going to take all of this and we're going to encode it using what's called a retriever model so it's called a retriever and what that will give us is a ton of these vectors where each Vector represents like a segment of our text so for example maybe we might have this little segment here followed by this segment and so on we're going to take all of those vector embeddings and we're going to put them into a vector database over here now we're going to be using pine cone for this so what we'll do is just put everything in Pine Cone and at that point we've actually built the retrieval pipeline we don't have the generative part of it yet but we'll do a retrieval pipeline so then what we can do is ask a question so ask a question over here it will be in natural language and what we'll do with that is actually also take that into the retriever model and that will output we'll maybe we'll output it over here that will output a single query vector okay or question Vector that would then be passed into pinecen here which will compare that query Vector to all of the previously encoded vectors and it will return a few of those that are the most relevant to our particular query Vector so it will bring these out and then we say okay these three items are the most relevant to your particular query and it's basing those on look at the concept or the idea behind the language being used it's not basing them on matching particular terms like keyword matching or anything like that it's actually basing it on the semantic understanding of the question and of the answers and of the relevant documents so we would take these and we'll bring them over here now over here we're going to have what's called a generator model so the generator model it can be a lot of different things one example that I kind of briefly mentioned is it could actually be something like gpt3 so you can have gpt3 here we're going to be using another one or another model called Bart that will generate everything for us just because this is open source and we can just run it in our code love notebook but you can use gpg 3 you can use to go hey you cannot use all these different types of models depending on what is you're wanting to do so we'd pass those relevant contacts or documents whatever you like to call them we pass those into our generator model alongside that we also want to pass in the question the original question one thing that I missed here is actually here foreign back into their original text format which we've stored in Pinecone so that will actually be the text and the same with the query so we're going to have the query and the context and we're going to feeding them into generator and that will then output as an answer in natural language format so let's actually jump straight into the code for building all of this so we're going to be working from this example over on the pine cone dots so it's pancone IO dots abstractive question answering and there'll be a link in the video as well and what we want to do is just opening collab over here that will open this so let's get started we need to install any dependencies so in here we have data sets pineco and sentence Transformers and pytorch and we'll jump into what each one of those does pretty soon okay once that is installed we come down here and we're going to just load and prepare our data set so we're taking these Wikipedia Snippets date step and this is coming from the hug and face data sets Hub so we're loading it like this and it's a pretty big data set so we're actually streaming that data by saying streaming equals true I think it's nine gigabytes so this will just allow us to load what we're using right now rather than loading the full thing in memory at once and then we Shuffle that data set randomly so we we're using a seat here just so you can replicate what I'm doing here let me run this and then we'll come down here and we can just show the first item or the first document from the data set so we just iterating through it we take the next item and we can see we have the ID and we saw and then where you know where where the text is actually being pulled from and we have article title section title and then we have the passage tips okay so this is the the document or the context and this is what we're going to be encoding so this is what we're going to be encoding and storing in our Vector database so what I'm going to do here is actually filter for only the documents that have history in the section title here so basically we just want history related documents so we do that now we can't check how many items we have there because we're using the streaming feature so that will just essentially stream everything and if it sees history it will lay through if not it will not let through but they're quite a few teeth passages in there so we're just going to filter out or we're going to choose the first 50 000 of those which is quite a bit now one thing I should make you aware of here is in your runtime just it should be GPU anyway but in case it's not here you can set your Hardware to use a GPU if it's on none it means you're using CPU and it will be a lot slower when we're embedding everything later on so we do want to make sure that we're using GPU okay so after that has completed we have our 50 000 documents all with history in the section title so if we take a look at the head here we can see that all knows and don't all say history specifically but they have history at least in the title like here okay so what we're going to do now is we'll need to embed and index all of these passages here or embed and sort all of them so to do that we're going to we'll need to initialize the pine cone index but I'm going to do that after initializing the retriever model so I'm going to scroll down to here come to the retrieve model and we're going to be using this Flex sentence embeddings or data sets V3 mpnet base model so this is basically one of the best sentence transform models you can use for basically anything so that's why it has all here it has been trained on I think a billion sentence pairs so it's a pretty good model to try and use whenever you're not sure which model to use so we initialize that okay it might take a moment to download okay and then one thing we will want to do is make sure we move this to a GPU so actually what we need to do is import torch now I want to say device equals Cuda if torch Cuda is available so this is saying if there's a cooler enabled GPU set the device to that otherwise we're going to use CPU okay now we can see what the device is and actually rather than moving the retriever to that device I'm going to come back up to the initialization here and I'm going to initialize it on that device to start with so like this okay now an important thing to note here is that we have the word embedding Dimension seven six eight so remember that and we'll come up here and we will initialize our pine cone index so the first thing we need to do is connect to our pine cone environment so we need an API key for that which is free so to get that we need to go to app.pinecone dot IO I want to say we will need to sign up or log in so I'm going to log in and once we've done that we'll just get a little loading screen here and then we should find something like this so on the top left up here you have your organization and then you have projects so one of those should say like your name and default project so I'm going to go over to that and in here I just have a list of the indexes that I currently have running now I think abstractive question answering is not in there so what I'm going to do is we're going to have to create it so we come over to API keys on the left here we copy the API key value come over to here and then we would just paste it into here I'm going to go and paste mine into a new variable so mine is sword in a new variable called API key so I initialize with that and what we're going to do is create a new index we're going to call it abstractive question answering and we are going to say if that index name does not exist then we create it now I remember I said to remember that dimensionality at number 768 before this is why because it's here we need that number to Align this number here to a line with the embedding dimensionality of our retriever model we can also check that using this so Retriever get sentence embedding dimension like so and we get 768 so we can actually take this and place it in here rather than hard coding it metric because the embedding vectors are normalized as we can see here we can actually use either dot product or cosine similarity here we're going to just stick cosine similarity and that will just take a moment for the index to be created okay once we have created it we will move on to this which is just connecting to our new index so let's scroll down and we will come down to the generating embeddings and upsetting so what we're going to do here is in batches of 64 we're going to extract our passage text so we'll have 64 of these passages although one time and we're going to encode them all using our retrieve model then what we're going to do is get metadata so that is simply the the text that we have in here so if I show you an example we have take this and then the DF and we're going to take first a few items and paste that so we're basically going to do this we're going to take all of that data that we have now data frame and for each one of our vectors so first one will be this we're going to attach that metadata to the vector and then here would create some unique IDs just count uh we could actually use the IDS themselves but this is just easier and we're going to add all those to a upset list which is just a list that contains two boards containing a each ID the vector embedding and the metadata related to that embedding and then we upset all of that so basically inside all into the pine cone Vector database then at the end here we're just going to check that we have all those vectors in the index and you can see here that it actually brought through five fifty thousand and one so maybe there's a duplicate in there I'm not too sure but we have all of those in there so I can try running this but it's basically just going to start from start again so you can see here I'm not going to wait until the end of that because it will take a little bit of time even when we're using a GPU on collab although actually not too long anyway I'm going to stop that and we'll just move straight on to the generator and we can at least just see from the pass runs at what it would be doing so the first thing we would do here is initialize the tokenizer and the model for our generator model and we're using a spot lfqa which is long-formal question hand string model okay so if you come up here we'll explain a little bit of what this model is so using the explain lycam 5 bar model which is just a sequence sequence model which has been trained using at Spain like M5 data set which is from Reddit and if we come down here we can see the format that we're going to be putting all of our text into this model so we're going to have our question which can be what we type we say like what is a sonic boom and then that's followed by context and then with each passage we proceed it with a p token like this and then we have the passage and then P token another passage and basically the model has been trained to read this sort of format and then generate a natural language answer based on this question and based on this information that we have provided it with so we come down here we would initialize it like that and then we're just going to create these to helper functions so this is just to help us query Pinecone so given a particular query we encode it so from text to a vector embedding or the query embedding is what we usually call it we query Pinecone like this this will return K many passages and it would return these what what we call the contacts or the passages or something along those lines one thing that is pretty important here is that we include the metadata because that includes the human readable text of those pastures that we're going to be feeding in and why do we need that because we are going to be formatting them in this string which is like what I showed you before we have the so the context here which is going to be the P token followed by the passage and then we concatenate all those together and then what we would do is create that format that you saw before with the question followed by the question and the context followed by those contacts with the p tokens in the in the middle or preceding each one so with those help functions we then move on to our query so we have our query when's the first electric power system built we can query pine cone and that will return these matches here so this is the response directly from Pinecone and we see that we have the passage text and we have some I think relevant passages in there so this is just returning just returning one here we use pretty print here so that we can more nicely visualize everything or print everything and then what we want to do is query or format our query so we have our query which is a question we just asked up here one's first electric power system built and then we also have what we return from Pinecone okay we and then we print what we get from there or what we will be producing so we have the question and you can see that same format that you saw before and then you have contacts and you have the P token followed by the passages so we write another function generate answer this is going to take our the formatted query here it's going to tokenize it using our bot tokenizer and then it's going to use a generator to generate a a prediction or generating answer okay so from there we that will output a load of token IDs which we obviously can't read so then we use this batch decode or the tokenizer decode to decode them into human readable text like that so if we then go ahead and actually run that we will see that we want to focus on this bit here the first electric power system was built in 1881 at gadaming in England I was powered by two oils and then and there so if we look at that answer or what we looked at here we can see that it is basically reformulating that interface in there into a more concise answer so we see in 1881 of gold arming in England and so on so that's pretty cool now what if we go a little further if we ask some more questions you say how was the first Wireless message sent and this time we're going to return five of these contacts so we're going to return more information and ideally this should give us give the BART generation model more information to produce an answer from so it should generally speaking be able to produce a better answer if we give it more of that information but not all the time in this case we say we see first Wireless meshes sent in 1866 so on and so on okay nice short answer which is good we set that by setting the max length up here at 14. and you know I don't know the answer to this question so what we can do is you know not just rely on the model to actually give us the answer which is a problem that we see a lot with the gbt 3 chat gbt and so on models we can actually have a look at what where this information is actually coming from so we can see here I think this is probably the most relevant part so this guy claimed to have transmitted an electrical signal through the atmosphere at this point right and I don't think AMD other contexts rarely give us any more information on there so we can see that according to this context and if we wanted we could provide a link back to where that was actually from that does at least seem to be true now this is probably a good example of when this is useful so if we ask a question like where did covid-19 originate and we get this like random answer and I think most of us probably know that this isn't this is kind of nonsense right so it's a zoonotic disease it's transmitted from one animal to another okay let's have a look at where this is coming from and we can see that all of these contacts don't actually contain anything about covid-19 and so we can pretty confident in saying that this is nonsense and simply the reason is that this model has never seen anything about covid-19 the BART generation model it hasn't seen anything about that because the training data it was trained on was from before that time and as well none of the contexts that we have indexed here contain anything about it either so it can be pretty useful to include that particularly when it comes to fact checking things like that and then let's finish your final few questions was Warren current I'm not going to check these uh but I'm pretty sure so this one is true first person on the Moon Neil Armstrong we I think all know that is true and what is NASA's most expensive project I think this one is possibly possibly true possibly not I can't remember but nonetheless we we get some pretty cool answers there so that's it for this video in this example walkthrough of abstractive or generative question answering I hope this has been useful and interesting so thank you very much for watching and I will see you again in the next one bye"

proofs = ["n that are related to our particular question and <highlight>We're also going to be able to use something called a generator model to generate a human natural language answer to our question based on these documents that we've retrieved from an external source<highlight> so we can think of it as a GPT model that is answ", "today we're going to talk about <highlight>Abstractive<highlight> or generative question and answering and we're go", "se we're going to be using text from Wikipedia so <highlight>We're going to take all of this and we're going to encode it using what's called a retriever model<highlight> so it's called a retriever and what that will giv"]

# TODO:
# given a string of "proof" and an array of subtitles, find the subtitle with biggest overlap with the proof
# required: find the time of the start of the proof, optional: find the time of the end of the proof

def get_time_for_proof_start_end(yt_captions, proof, search_start=True):
  highligh = proof.split('<highlight>')[1]
  if not highligh:
    return
  hightlight_processed = highligh.lower()
  highlight_parts = hightlight_processed.split(' ')
  highligh_parts_in_segments_distribution = {}
  for idx, _ in enumerate(highlight_parts):
    searched_phrase = ''
    all_segments_with_hightligh = []
    if search_start:
      if idx == 0:
        searched_phrase = highlight_parts[0]
      else:
        searched_phrase = ' '.join(highlight_parts[0:idx])
    else:
      if idx == 0:
        searched_phrase = highlight_parts[-1:][0]
      else:
        idx_from_end = -1 + idx * -1
        searched_phrase = ' '.join(highlight_parts[idx_from_end:])
    for segment in yt_captions:
      if searched_phrase in segment['text']:
        all_segments_with_hightligh.append(segment)
    highligh_parts_in_segments_distribution[idx] = all_segments_with_hightligh
    # we need a segment which contains the longest highlight part
    # the longer the hightlight part, the less segments will coincide with it
    # and then after adding the next word the hightligh will not be found in any segment
    # so we need the step with 0 found after not-0, and take this not-0 value (expected to be only 1)
    if len(highligh_parts_in_segments_distribution[idx]) == 0 and (idx - 1 >= 0) and len(highligh_parts_in_segments_distribution[idx-1]) > 0:
      the_segment = highligh_parts_in_segments_distribution[idx-1][0]
      if search_start:
        print(f'[get_time_for_proof_start_end] found phrase "{searched_phrase}" (start={search_start}) in segment at {the_segment["start"]}')
        return the_segment['start']
      else:
        segment_ends_at = the_segment['start'] + the_segment['duration']
        print(f'[get_time_for_proof_start_end] found phrase "{searched_phrase}" (start={search_start}) in segment at {segment_ends_at}')
        return segment_ends_at
  print('[get_time_for_proof_start_end] failed to find start/end for the proof "{proof}"')
  return None

def get_proof_time(yt_captions, proof):
  start = get_time_for_proof_start_end(yt_captions, proof)
  end = get_time_for_proof_start_end(yt_captions, proof, False)
  last_caption = yt_captions[-1:][0]
  video_length = last_caption['start'] + last_caption['duration']
  YT_SEGMENT_BORDERS_SEC = 3
  start_with_borders = start - YT_SEGMENT_BORDERS_SEC if start - YT_SEGMENT_BORDERS_SEC >= 0 else 0
  end_with_borders = end + YT_SEGMENT_BORDERS_SEC if end + YT_SEGMENT_BORDERS_SEC <= video_length else video_length
  return { 'start': start_with_borders, 'end': end_with_borders }

from misc import yt_captions_example
# with open('misc/yt_captions_example.json') as f:
#     data = json.load(f)

# print(get_time_for_proof_start(yt_captions_example.captions, proofs[0]))
# print(get_time_for_proof_start(yt_captions_example.captions, proofs[0], False))
print(get_proof_time(yt_captions_example.captions, proofs[0]))
# a = ['hey', 'wei', 'my']
# print(a[-1:])
# print(a[-2:])
# print(a[-3:])
# print(a[-4:])